{
  "authors": [
    "Feb"
  ],
  "date_download": "2018-02-06T16:58:23",
  "date_modify": null,
  "date_publish": "2018-02-05T12:00:04",
  "description": "Google\u2019s Pixel Visual Core, the hidden image-processing chip inside the Pixel 2 family of phones, is getting an update today that lets it work its machine learning magic in third-party apps.",
  "filename": "https%3A%2F%2Fwww.theverge.com%2Fcircuitbreaker%2F2018%2F2%2F5%2F16973286%2Fgoogle-pixel-visual-core-pixel-2-app.json",
  "image_url": "https://cdn.vox-cdn.com/thumbor/_rNA3hhX0JdBfuW7MQpL8SA8FYE=/0x68:2040x1136/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/9474517/jbareham_171013_2050_0214.jpg",
  "language": null,
  "localpath": null,
  "source_domain": "www.theverge.com",
  "text": "Google\u2019s Pixel Visual Core, the hidden image-processing chip inside the Pixel 2 family of phones, is getting an update today that lets it work its machine learning magic in third-party apps. Already enabled via Android 8.1 for the Pixel 2\u2019s main camera app, the Visual Core is now going to be operational within any other camera app that employs the relevant Google APIs. That means your Instagram photography and Snapchat Stories will get the benefit of the same improvements in processing speed and efficiency.\nI have been using a Google Pixel 2 XL since before the Android 8.1 update that initially flipped the Visual Core to being active, and I can\u2019t say I\u2019ve noticed a huge difference in the speed or operation of the camera. It was sterling before 8.1, and it\u2019s been the same since. But the way Google explains it, the Visual Core is likely to be more helpful and impressive in third-party apps because it will allow the company to run its proprietary HDR+ algorithm in those other apps:\n\u201cPixel Visual Core is built to do heavy-lifting image processing while using less power, which saves battery. That means we\u2019re able to use that additional computing power to improve the quality of your pictures by running the HDR+ algorithm.\u201d\nHere are a couple of examples of the Pixel Visual Core\u2019s impact in practice:\nThis first example shows off the thing Google\u2019s photography algorithms are most famous for, which is the extraction of additional detail in shadows without butchering the photo with excessive noise or blurring.\nThis other example shows the downside, however, as the less-exposed, Visual Core-deprived shot appears more atmospheric and pleasing to the eye. There\u2019s a subtle light halo around the roof of the house and the edges of the trees that feels artificial (because it is). The less-processed pic conveys a better sense of the sunset being captured.\nThe Android software update that activates the Pixel Visual Core for third-party apps is rolling out over the next few days, starting immediately. Google also promises to kick out some fresh augmented reality stickers for the Pixel camera later this week, themed around winter sports and timed to coincide with the Winter Olympics.",
  "title": "Google enables Pixel Visual Core for better Instagram, Snapchat photos",
  "title_page": null,
  "title_rss": null,
  "url": "https://www.theverge.com/circuitbreaker/2018/2/5/16973286/google-pixel-visual-core-pixel-2-app",
  "dId": "6d7866f5d80f7ccb9eada3cde7c6caea061d4af47e8517e4a7b97fc0",
  "newsCluster": {
    "CategoryId": 3,
    "Category": "technology",
    "TopicId": 1,
    "Topic": "unspecific",
    "EventId": 58,
    "Event": "Google_enables_Pixel_Visual_Core"
  }
}